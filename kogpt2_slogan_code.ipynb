{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "def cleaning(s):\r\n",
    "  s = str(s)\r\n",
    "  s = re.sub(', ', ',', s)\r\n",
    "  s = re.sub(',', ' ', s)\r\n",
    "  return s\r\n",
    "\r\n",
    "df = pd.read_csv(\"./datasets/final.csv\", encoding=\"utf-8\")\r\n",
    "df = df.dropna()\r\n",
    "text_data = open('./datasets/slogans.txt', 'w', encoding=\"utf-8\")\r\n",
    "for idx, item in df.iterrows():\r\n",
    "  slogans = cleaning(item['company']) + ', ' + item['slogan']+ '\\n'\r\n",
    "  text_data.write(slogans)\r\n",
    "text_data.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "from transformers import Trainer, TrainingArguments\r\n",
    "from transformers import PreTrainedTokenizerFast\r\n",
    "\r\n",
    "\r\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\r\n",
    "    dataset = TextDataset(\r\n",
    "        tokenizer = tokenizer,\r\n",
    "        file_path = file_path,\r\n",
    "        block_size = block_size,\r\n",
    "    )\r\n",
    "    return dataset\r\n",
    "\r\n",
    "\r\n",
    "def load_data_collator(tokenizer, mlm = False):\r\n",
    "    data_collator = DataCollatorForLanguageModeling(\r\n",
    "        tokenizer=tokenizer, \r\n",
    "        mlm=mlm,\r\n",
    "    )\r\n",
    "    return data_collator\r\n",
    "\r\n",
    "def train(train_file_path,model_name,\r\n",
    "          output_dir,\r\n",
    "          overwrite_output_dir,\r\n",
    "          per_device_train_batch_size,\r\n",
    "          num_train_epochs,\r\n",
    "          save_steps):\r\n",
    "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\r\n",
    "                bos_token='</s>', eos_token='</s>', unk_token='<unk>',\r\n",
    "                pad_token='<pad>', mask_token='<mask>')\r\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\r\n",
    "  data_collator = load_data_collator(tokenizer)\r\n",
    "\r\n",
    "  tokenizer.save_pretrained(output_dir, legacy_format=False)\r\n",
    "   \r\n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\r\n",
    "\r\n",
    "  model.save_pretrained(output_dir)\r\n",
    "\r\n",
    "  training_args = TrainingArguments(\r\n",
    "          output_dir=output_dir,\r\n",
    "          overwrite_output_dir=overwrite_output_dir,\r\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "          num_train_epochs=num_train_epochs,\r\n",
    "      )\r\n",
    "\r\n",
    "  trainer = Trainer(\r\n",
    "          model=model,\r\n",
    "          args=training_args,\r\n",
    "          data_collator=data_collator,\r\n",
    "          train_dataset=train_dataset,\r\n",
    "  )\r\n",
    "      \r\n",
    "  trainer.train()\r\n",
    "  trainer.save_model()\r\n",
    "\r\n",
    "train_file_path = './datasets/slogans.txt'\r\n",
    "model_name = 'skt/kogpt2-base-v2'\r\n",
    "output_dir = './models2'\r\n",
    "overwrite_output_dir = False\r\n",
    "per_device_train_batch_size = 8\r\n",
    "num_train_epochs = 5.0\r\n",
    "save_steps = 500\r\n",
    "\r\n",
    "train(\r\n",
    "    train_file_path=train_file_path,\r\n",
    "    model_name=model_name,\r\n",
    "    output_dir=output_dir,\r\n",
    "    overwrite_output_dir=overwrite_output_dir,\r\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "    num_train_epochs=num_train_epochs,\r\n",
    "    save_steps=save_steps\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tyler\\anaconda3\\envs\\AI_exam\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 500/1020 [01:43<01:47,  4.83it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.4771, 'learning_rate': 2.5490196078431373e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1000/1020 [03:27<00:04,  4.88it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.6098, 'learning_rate': 9.80392156862745e-07, 'epoch': 4.9}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1020/1020 [03:33<00:00,  4.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 213.6555, 'train_samples_per_second': 4.774, 'epoch': 5.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\r\n",
    "\r\n",
    "def load_model(model_path):\r\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def load_tokenizer(tokenizer_path):\r\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\r\n",
    "    return tokenizer\r\n",
    "\r\n",
    "\r\n",
    "def generate_text(sequence, max_length):\r\n",
    "    model_path = \"./models2\"\r\n",
    "    model = load_model(model_path)\r\n",
    "    tokenizer = load_tokenizer(model_path)\r\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\r\n",
    "    final_outputs = model.generate(\r\n",
    "        ids,\r\n",
    "        do_sample=True,\r\n",
    "        max_length=max_length,\r\n",
    "        pad_token_id=model.config.pad_token_id,\r\n",
    "        top_k=50,\r\n",
    "        top_p=0.95,\r\n",
    "    )\r\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\r\n",
    "\r\n",
    "# sequence = input()\r\n",
    "# max_len = int(input())\r\n",
    "\r\n",
    "input = 'ì˜¤í”ˆì›”ë“œ ê²Œì„'\r\n",
    "\r\n",
    "sequence = input\r\n",
    "max_len = 50\r\n",
    "\r\n",
    "print('input :' + sequence)\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    print(generate_text(sequence, max_len))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input :ì˜¤í”ˆì›”ë“œ ê²Œì„\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ë‚´ê°€ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸°ë¥¼ ì°¾ì•„<eos>í…Œì¼ì¦ˆëŸ°ë„ˆ, ëª¨ë°”ì¼ RPG ê²Œì„ë‚´ ì†ì—ì„œ ì‹œì‘í•˜ëŠ” ìƒˆë¡œìš´ MMORPG<eos>í…Œë¼M, ëª¨ë°”ì¼ MMORPG ê²Œì„M\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, í•œêµ­ ì´ìš©ìë¥¼ ìœ„í•œ ìµœê³ ì˜ ì„ íƒ<eos>ë¦¬ë‹ˆì§€2, ë¦¬ë‹ˆì§€2 ë ˆë³¼ë£¨ì…˜, ë¦¬ë‹ˆì§€2ì˜ íƒ„ìƒ<eos>ë¦¬ë‹ˆì§€2, ë¦¬ë‹ˆì§€2ë ˆë³¼ë£¨ì…˜, ë¦¬\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, í•œêµ­ì„ ë„˜ì–´ ì„¸ê³„ì¸ì˜ ê²Œì„ ì¶•ì œ<eos>2016ì„œìš¸ëª¨í„°ì‡¼2016, ìë™ì°¨ ë¬¸í™”ì¶•ì œí•˜ëŠ˜ ì•„ë˜ ì–´ë””ë“  ììœ ë¡­ê²Œ<eos>2018ì„œìš¸ëª¨í„°ì‡¼2018, ìë™ì°¨ ë¬¸í™”ì¶•ì œê¿ˆì´\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ëŒ€í•œë¯¼êµ­ ìœ ì €ì˜ ê²Œì„ìƒëŒ€ê°€ ë˜ë‹¤<eos>ë„·ë§ˆë¸” ëª¨ë°”ì¼ ì „ëµ RPGì„¸ìƒì´ ë¶€ë¥¸ ì˜ì›…ë“¤<eos>ë„·ë§ˆë¸” MMORPGëŒ€í•œë¯¼êµ­ì˜ ì˜ì›…ë“¤ì˜ ëª¨ë°”ì¼ ì „ìŸ<\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, í•œêµ­ ê²Œì„ë‹¹ì‹ ì˜ ëª¨ë“  íŒíƒ€ì§€ ëª¨í—˜<eos>ë¼ê·¸ë‚˜ë¡œí¬ ì˜¤ë¦¬ì§„, ëª¨ë°”ì¼ MMORPG ê²Œì„ë‚˜ë§Œì˜ ê³³ìœ¼ë¡œ ë– ë‚˜<eos>ë¼ê·¸ë‚˜ë¡œí¬ ì˜¤ë¦¬ì§„, ëª¨ë°”ì¼ MMORP\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ê·¸ë¦¬ê³  í¼ë¸”ë¦¬ì…”ê°€ ë˜ë‹¤<eos>ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ ë©”ê°€ë°•ìŠ¤, í”„ë¦¬ë¯¸ì—„ ì½˜í…ì¸  ì œê³µì—…ê²¨ìš¸ì€ ë¡œë§¨í‹±í•˜ê²Œ<eos>ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ íƒ­ì†Œë‹‰, í”„ë¦¬ë¯¸ì—„ ì½˜í…ì¸ \n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, í•œêµ­ì˜ ëª¨ë“  ìœ ì €ëŠ” WITH MOVE<eos>KTê¸°ê°€ IOT, ì¸ê³µì§€ëŠ¥ í™ˆ IoT ì„œë¹„ìŠ¤ì•„ì´ë“¤ì´ IOT í•˜ë‹ˆê¹Œ ê¸°ê°€ IOT<eos>\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ëŒ€í•œë¯¼êµ­ì„ í•˜ë‚˜ë¡œ ì´ì–´ê°€ë‹¤<eos>ì¼ê³± ê°œì˜ëŒ€ì£„, ì˜ì›…ë“¤ì´ì—¬.ì—¬ëŸ¬ë¶„ì´ ì›í•˜ë˜ ì„¸ìƒ<eos>ì–´ë“œë²¤ì²˜, SF ëª¨ë°”ì¼ RPG ê²Œì„ì˜ì›…ë“¤ì´ì—¬, ì´\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ì—”í„°í…Œì¸ë¨¼íŠ¸ ê²Œì„ìƒˆë¡­ê²Œ ë§Œë‚˜ëŠ” ìƒˆë¡œìš´ ì„¸ê³„<eos>ì•„ë°œë¡  ëª¨ë°”ì¼ ë ˆì´ë“œ ì—…ë°ì´íŠ¸, ëª¨ë°”ì¼ ê²Œì„ì¦ê±°ì›€ì„ ìœ„í•œ ëª¨í—˜<eos>ì•„ë°œë¡  ëª¨ë°”ì¼ ë ˆì´ë“œ ì—…ë°ì´íŠ¸,\n",
      "None\n",
      "ì˜¤í”ˆì›”ë“œ ê²Œì„, ëŒ€í•œë¯¼êµ­ì— ì—†ë˜ ì„¸ê³„<eos>ì‚¼êµ­ë¸”ë ˆì´ë“œ, ëª¨ë°”ì¼ MMORPG ê²Œì„í•œ ë²ˆì€ ì‚¼êµ­ì§€ë‹¤, í•œë²ˆì€ ì‚¼êµ­ì§€ë‹¤<eos>ì‚¼êµ­ë¸”ë ˆì´ë“œ2, ëª¨ë°”ì¼ MMORPG ê²Œì„\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import requests\r\n",
    "import json\r\n",
    "\r\n",
    "company = \"ì–´ìŒ”ì”¬í¬ë¦¬ë“œ ë°œí• ë¼\" #(ìµœëŒ€ 10ê¸€ì)\r\n",
    "context = \"ì˜¤í”ˆì›”ë“œ ê²Œì„\" #(ìµœëŒ€ 15ê¸€ì)\r\n",
    "\r\n",
    "input = company + ',' + context + ','\r\n",
    "\r\n",
    "r = requests.post(\r\n",
    "    'https://train-nqhy6iii8xyt5739th8f-gpt2-train-teachable-ainize.endpoint.ainize.ai/predictions/gpt-2-ko-small-finetune', #ê¸°ì—…ì„¤ëª…, ìŠ¬ë¡œê±´\r\n",
    "    headers = {'Content-Type' : 'application/json'\r\n",
    "               },\r\n",
    "    data=json.dumps({\r\n",
    "  \"text\": input,\r\n",
    "  \"num_samples\": 10,\r\n",
    "  \"length\": 20\r\n",
    "    }))\r\n",
    "\r\n",
    "#print(r.json())\r\n",
    "print('input :' + input)\r\n",
    "print('='*50)\r\n",
    "for slogan in r.json():\r\n",
    "    slogan = slogan.split('\\n')[0]\r\n",
    "    slogan = slogan.split(',')[2:]\r\n",
    "    slogan = ', '.join(slogan)\r\n",
    "    if slogan :\r\n",
    "        print(slogan)\r\n",
    "        print('='*50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input :ì–´ìŒ”ì”¬í¬ë¦¬ë“œ ë°œí• ë¼,ì˜¤í”ˆì›”ë“œ ê²Œì„,\n",
      "==================================================\n",
      "ìƒˆë¡œìš´ ì‹œì‘ì´ ì‹œì‘ëœë‹¤ \n",
      "==================================================\n",
      "ëª¨ë“  ê²Œ ì‹ ê¸°ë£¨ì²˜ëŸ¼ \n",
      "==================================================\n",
      "ë‹¤í•¨ê»˜ \n",
      "==================================================\n",
      "ì›”ë“œì— ì—†ë˜ ìƒˆë¡œìš´ ì¦ê±°ì›€ì´ ìƒê²¼ë‹¤ \n",
      "==================================================\n",
      "MMORPG ê²Œì„ \n",
      "==================================================\n",
      "ìƒˆë¡œìš´ ì„¸ê³„ \n",
      "==================================================\n",
      "ì´í† ë¡ ê¸´ë°•í•œ ëª¨í—˜ì€ í•œë²ˆë„ ê²½í—˜í•´ë³´ì§€ ëª»í•œ \n",
      "==================================================\n",
      "ëª¨ë°”ì¼ ëŒ€ì „ ê²Œì„ì—ì„œ ê°€ì¥ í° ì¦ê±°ì›€ì´ ìˆë‹¤ \n",
      "==================================================\n",
      "ìƒˆë¡œìš´ ëª¨í—˜ì˜ ì‹œì‘ \n",
      "==================================================\n",
      "ë‹¹ì‹ ë§Œ ì•„ëŠ” ë…€ì„ \n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}