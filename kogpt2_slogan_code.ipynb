{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "def cleaning(s):\r\n",
    "  s = str(s)\r\n",
    "  s = re.sub(', ', ',', s)\r\n",
    "  s = re.sub(',', ' ', s)\r\n",
    "  return s\r\n",
    "\r\n",
    "df = pd.read_csv(\"./datasets/final.csv\", encoding=\"utf-8\")\r\n",
    "df = df.dropna()\r\n",
    "text_data = open('./datasets/slogans.txt', 'w', encoding=\"utf-8\")\r\n",
    "for idx, item in df.iterrows():\r\n",
    "  slogans = cleaning(item['company']) + ', ' + item['slogan']+ '\\n'\r\n",
    "  text_data.write(slogans)\r\n",
    "text_data.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "from transformers import Trainer, TrainingArguments\r\n",
    "from transformers import PreTrainedTokenizerFast\r\n",
    "\r\n",
    "\r\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\r\n",
    "    dataset = TextDataset(\r\n",
    "        tokenizer = tokenizer,\r\n",
    "        file_path = file_path,\r\n",
    "        block_size = block_size,\r\n",
    "    )\r\n",
    "    return dataset\r\n",
    "\r\n",
    "\r\n",
    "def load_data_collator(tokenizer, mlm = False):\r\n",
    "    data_collator = DataCollatorForLanguageModeling(\r\n",
    "        tokenizer=tokenizer, \r\n",
    "        mlm=mlm,\r\n",
    "    )\r\n",
    "    return data_collator\r\n",
    "\r\n",
    "def train(train_file_path,model_name,\r\n",
    "          output_dir,\r\n",
    "          overwrite_output_dir,\r\n",
    "          per_device_train_batch_size,\r\n",
    "          num_train_epochs,\r\n",
    "          save_steps):\r\n",
    "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\r\n",
    "                bos_token='</s>', eos_token='</s>', unk_token='<unk>',\r\n",
    "                pad_token='<pad>', mask_token='<mask>')\r\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\r\n",
    "  data_collator = load_data_collator(tokenizer)\r\n",
    "\r\n",
    "  tokenizer.save_pretrained(output_dir, legacy_format=False)\r\n",
    "   \r\n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\r\n",
    "\r\n",
    "  model.save_pretrained(output_dir)\r\n",
    "\r\n",
    "  training_args = TrainingArguments(\r\n",
    "          output_dir=output_dir,\r\n",
    "          overwrite_output_dir=overwrite_output_dir,\r\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "          num_train_epochs=num_train_epochs,\r\n",
    "      )\r\n",
    "\r\n",
    "  trainer = Trainer(\r\n",
    "          model=model,\r\n",
    "          args=training_args,\r\n",
    "          data_collator=data_collator,\r\n",
    "          train_dataset=train_dataset,\r\n",
    "  )\r\n",
    "      \r\n",
    "  trainer.train()\r\n",
    "  trainer.save_model()\r\n",
    "\r\n",
    "train_file_path = './datasets/slogans.txt'\r\n",
    "model_name = 'skt/kogpt2-base-v2'\r\n",
    "output_dir = './models2'\r\n",
    "overwrite_output_dir = False\r\n",
    "per_device_train_batch_size = 8\r\n",
    "num_train_epochs = 5.0\r\n",
    "save_steps = 500\r\n",
    "\r\n",
    "train(\r\n",
    "    train_file_path=train_file_path,\r\n",
    "    model_name=model_name,\r\n",
    "    output_dir=output_dir,\r\n",
    "    overwrite_output_dir=overwrite_output_dir,\r\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "    num_train_epochs=num_train_epochs,\r\n",
    "    save_steps=save_steps\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tyler\\anaconda3\\envs\\AI_exam\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      " 49%|████▉     | 500/1020 [01:43<01:47,  4.83it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.4771, 'learning_rate': 2.5490196078431373e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 98%|█████████▊| 1000/1020 [03:27<00:04,  4.88it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.6098, 'learning_rate': 9.80392156862745e-07, 'epoch': 4.9}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1020/1020 [03:33<00:00,  4.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 213.6555, 'train_samples_per_second': 4.774, 'epoch': 5.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\r\n",
    "\r\n",
    "def load_model(model_path):\r\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def load_tokenizer(tokenizer_path):\r\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\r\n",
    "    return tokenizer\r\n",
    "\r\n",
    "\r\n",
    "def generate_text(sequence, max_length):\r\n",
    "    model_path = \"./models2\"\r\n",
    "    model = load_model(model_path)\r\n",
    "    tokenizer = load_tokenizer(model_path)\r\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\r\n",
    "    final_outputs = model.generate(\r\n",
    "        ids,\r\n",
    "        do_sample=True,\r\n",
    "        max_length=max_length,\r\n",
    "        pad_token_id=model.config.pad_token_id,\r\n",
    "        top_k=50,\r\n",
    "        top_p=0.95,\r\n",
    "    )\r\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\r\n",
    "\r\n",
    "# sequence = input()\r\n",
    "# max_len = int(input())\r\n",
    "\r\n",
    "input = '오픈월드 게임'\r\n",
    "\r\n",
    "sequence = input\r\n",
    "max_len = 50\r\n",
    "\r\n",
    "print('input :' + sequence)\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    print(generate_text(sequence, max_len))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input :오픈월드 게임\n",
      "오픈월드 게임, 내가 하고 싶은 이야기를 찾아<eos>테일즈런너, 모바일 RPG 게임내 손에서 시작하는 새로운 MMORPG<eos>테라M, 모바일 MMORPG 게임M\n",
      "None\n",
      "오픈월드 게임, 한국 이용자를 위한 최고의 선택<eos>리니지2, 리니지2 레볼루션, 리니지2의 탄생<eos>리니지2, 리니지2레볼루션, 리\n",
      "None\n",
      "오픈월드 게임, 한국을 넘어 세계인의 게임 축제<eos>2016서울모터쇼2016, 자동차 문화축제하늘 아래 어디든 자유롭게<eos>2018서울모터쇼2018, 자동차 문화축제꿈이\n",
      "None\n",
      "오픈월드 게임, 대한민국 유저의 게임상대가 되다<eos>넷마블 모바일 전략 RPG세상이 부른 영웅들<eos>넷마블 MMORPG대한민국의 영웅들의 모바일 전쟁<\n",
      "None\n",
      "오픈월드 게임, 한국 게임당신의 모든 판타지 모험<eos>라그나로크 오리진, 모바일 MMORPG 게임나만의 곳으로 떠나<eos>라그나로크 오리진, 모바일 MMORP\n",
      "None\n",
      "오픈월드 게임, 그리고 퍼블리셔가 되다<eos>스마일게이트 메가박스, 프리미엄 콘텐츠 제공업겨울은 로맨틱하게<eos>스마일게이트 탭소닉, 프리미엄 콘텐츠\n",
      "None\n",
      "오픈월드 게임, 한국의 모든 유저는 WITH MOVE<eos>KT기가 IOT, 인공지능 홈 IoT 서비스아이들이 IOT 하니까 기가 IOT<eos>\n",
      "None\n",
      "오픈월드 게임, 대한민국을 하나로 이어가다<eos>일곱 개의대죄, 영웅들이여.여러분이 원하던 세상<eos>어드벤처, SF 모바일 RPG 게임영웅들이여, 이\n",
      "None\n",
      "오픈월드 게임, 엔터테인먼트 게임새롭게 만나는 새로운 세계<eos>아발론 모바일 레이드 업데이트, 모바일 게임즐거움을 위한 모험<eos>아발론 모바일 레이드 업데이트,\n",
      "None\n",
      "오픈월드 게임, 대한민국에 없던 세계<eos>삼국블레이드, 모바일 MMORPG 게임한 번은 삼국지다, 한번은 삼국지다<eos>삼국블레이드2, 모바일 MMORPG 게임\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import requests\r\n",
    "import json\r\n",
    "\r\n",
    "company = \"어쌔씬크리드 발할라\" #(최대 10글자)\r\n",
    "context = \"오픈월드 게임\" #(최대 15글자)\r\n",
    "\r\n",
    "input = company + ',' + context + ','\r\n",
    "\r\n",
    "r = requests.post(\r\n",
    "    'https://train-nqhy6iii8xyt5739th8f-gpt2-train-teachable-ainize.endpoint.ainize.ai/predictions/gpt-2-ko-small-finetune', #기업설명, 슬로건\r\n",
    "    headers = {'Content-Type' : 'application/json'\r\n",
    "               },\r\n",
    "    data=json.dumps({\r\n",
    "  \"text\": input,\r\n",
    "  \"num_samples\": 10,\r\n",
    "  \"length\": 20\r\n",
    "    }))\r\n",
    "\r\n",
    "#print(r.json())\r\n",
    "print('input :' + input)\r\n",
    "print('='*50)\r\n",
    "for slogan in r.json():\r\n",
    "    slogan = slogan.split('\\n')[0]\r\n",
    "    slogan = slogan.split(',')[2:]\r\n",
    "    slogan = ', '.join(slogan)\r\n",
    "    if slogan :\r\n",
    "        print(slogan)\r\n",
    "        print('='*50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input :어쌔씬크리드 발할라,오픈월드 게임,\n",
      "==================================================\n",
      "새로운 시작이 시작된다 \n",
      "==================================================\n",
      "모든 게 신기루처럼 \n",
      "==================================================\n",
      "다함께 \n",
      "==================================================\n",
      "월드에 없던 새로운 즐거움이 생겼다 \n",
      "==================================================\n",
      "MMORPG 게임 \n",
      "==================================================\n",
      "새로운 세계 \n",
      "==================================================\n",
      "이토록 긴박한 모험은 한번도 경험해보지 못한 \n",
      "==================================================\n",
      "모바일 대전 게임에서 가장 큰 즐거움이 있다 \n",
      "==================================================\n",
      "새로운 모험의 시작 \n",
      "==================================================\n",
      "당신만 아는 녀석 \n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}